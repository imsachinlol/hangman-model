{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d6d92ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Hangman HMM Training\n",
      "==================================================\n",
      "Loading corpus...\n",
      "Loaded 49979 words\n",
      "Training models for 24 different word lengths...\n",
      "Training length 11: 5452 words\n",
      "Training length 6: 3755 words\n",
      "Training length 9: 6787 words\n",
      "Training length 16: 698 words\n",
      "Training length 14: 2019 words\n",
      "Training length 10: 6465 words\n",
      "Training length 8: 6348 words\n",
      "Training length 12: 4292 words\n",
      "Training length 13: 3094 words\n",
      "Training length 5: 2340 words\n",
      "Training length 18: 174 words\n",
      "Training length 4: 1169 words\n",
      "Training length 3: 388 words\n",
      "Training length 7: 5111 words\n",
      "Training length 15: 1226 words\n",
      "Training length 17: 375 words\n",
      "Training length 22: 8 words\n",
      "Training length 19: 88 words\n",
      "Training length 2: 84 words\n",
      "Training length 20: 40 words\n",
      "Training length 21: 16 words\n",
      "Training length 23: 3 words\n",
      "Training length 24: 1 words\n",
      "Training complete!\n",
      "Model saved to hangman_hmm_model.pkl\n",
      "\n",
      "==================================================\n",
      "Testing HMM\n",
      "==================================================\n",
      "\n",
      "Masked word: _ppl_\n",
      "Guessed: {'e', 's'}\n",
      "Top 5 predictions: [('a', '0.1192'), ('y', '0.0804'), ('o', '0.0784'), ('l', '0.0713'), ('t', '0.0625')]\n",
      "\n",
      "Masked word: h_ll_\n",
      "Guessed: {'e'}\n",
      "Top 5 predictions: [('a', '0.1832'), ('o', '0.1235'), ('i', '0.0963'), ('y', '0.0766'), ('l', '0.0667')]\n",
      "\n",
      "Masked word: _____\n",
      "Guessed: set()\n",
      "Top 5 predictions: [('a', '0.1063'), ('e', '0.0976'), ('o', '0.0692'), ('r', '0.0672'), ('i', '0.0633')]\n",
      "\n",
      "Masked word: pro___m\n",
      "Guessed: {'e', 'a', 'i'}\n",
      "Top 5 predictions: [('n', '0.0999'), ('o', '0.0988'), ('l', '0.0977'), ('r', '0.0919'), ('t', '0.0899')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "class HangmanHMM:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model for Hangman letter prediction.\n",
    "    Trains separate models for each word length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}  # Dictionary of models by word length\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.letter_to_idx = {letter: idx for idx, letter in enumerate(self.alphabet)}\n",
    "        self.idx_to_letter = {idx: letter for idx, letter in enumerate(self.alphabet)}\n",
    "        \n",
    "    def train(self, corpus_file):\n",
    "        \"\"\"Train HMM on corpus file.\"\"\"\n",
    "        print(\"Loading corpus...\")\n",
    "        with open(corpus_file, 'r') as f:\n",
    "            words = [line.strip().lower() for line in f if line.strip()]\n",
    "        \n",
    "        # Filter only alphabetic words\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        \n",
    "        print(f\"Loaded {len(words)} words\")\n",
    "        \n",
    "        # Group words by length\n",
    "        words_by_length = defaultdict(list)\n",
    "        for word in words:\n",
    "            words_by_length[len(word)].append(word)\n",
    "        \n",
    "        print(f\"Training models for {len(words_by_length)} different word lengths...\")\n",
    "        \n",
    "        # Train a model for each word length\n",
    "        for length, word_list in words_by_length.items():\n",
    "            if length < 2:  # Skip very short words\n",
    "                continue\n",
    "            print(f\"Training length {length}: {len(word_list)} words\")\n",
    "            self.models[length] = self._train_length_model(word_list, length)\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    def _train_length_model(self, words, length):\n",
    "        \"\"\"Train HMM for specific word length.\"\"\"\n",
    "        model = {\n",
    "            'length': length,\n",
    "            'position_probs': np.zeros((length, 26)),  # P(letter | position)\n",
    "            'transition_probs': np.zeros((26, 26)),     # P(letter_t | letter_t-1)\n",
    "            'initial_probs': np.zeros(26),              # P(first letter)\n",
    "            'word_count': len(words)\n",
    "        }\n",
    "        \n",
    "        # Count letter occurrences at each position\n",
    "        position_counts = np.zeros((length, 26))\n",
    "        \n",
    "        for word in words:\n",
    "            # Initial letter\n",
    "            if len(word) > 0:\n",
    "                model['initial_probs'][self.letter_to_idx[word[0]]] += 1\n",
    "            \n",
    "            # Position-specific counts\n",
    "            for pos, letter in enumerate(word):\n",
    "                if letter in self.letter_to_idx:\n",
    "                    position_counts[pos][self.letter_to_idx[letter]] += 1\n",
    "            \n",
    "            # Transition counts (bigrams)\n",
    "            for i in range(len(word) - 1):\n",
    "                curr_letter = word[i]\n",
    "                next_letter = word[i + 1]\n",
    "                if curr_letter in self.letter_to_idx and next_letter in self.letter_to_idx:\n",
    "                    curr_idx = self.letter_to_idx[curr_letter]\n",
    "                    next_idx = self.letter_to_idx[next_letter]\n",
    "                    model['transition_probs'][curr_idx][next_idx] += 1\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        # Position probabilities with smoothing\n",
    "        for pos in range(length):\n",
    "            total = position_counts[pos].sum() + 26  # Laplace smoothing\n",
    "            model['position_probs'][pos] = (position_counts[pos] + 1) / total\n",
    "        \n",
    "        # Initial probabilities with smoothing\n",
    "        total = model['initial_probs'].sum() + 26\n",
    "        model['initial_probs'] = (model['initial_probs'] + 1) / total\n",
    "        \n",
    "        # Transition probabilities with smoothing\n",
    "        for i in range(26):\n",
    "            total = model['transition_probs'][i].sum() + 26\n",
    "            model['transition_probs'][i] = (model['transition_probs'][i] + 1) / total\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_letter_probabilities(self, masked_word, guessed_letters):\n",
    "        \"\"\"\n",
    "        Get probability distribution over letters given current game state.\n",
    "        \n",
    "        Args:\n",
    "            masked_word: str, e.g., \"_ppl_\"\n",
    "            guessed_letters: set of already guessed letters\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of probabilities for each letter (26 dimensions)\n",
    "        \"\"\"\n",
    "        length = len(masked_word)\n",
    "        \n",
    "        if length not in self.models:\n",
    "            # Fallback to general letter frequency\n",
    "            return self._get_frequency_based_probs(guessed_letters)\n",
    "        \n",
    "        model = self.models[length]\n",
    "        letter_scores = np.zeros(26)\n",
    "        \n",
    "        # Aggregate probabilities from each blank position\n",
    "        for pos, char in enumerate(masked_word):\n",
    "            if char == '_':\n",
    "                # Add position-based probability\n",
    "                letter_scores += model['position_probs'][pos]\n",
    "                \n",
    "                # Consider transitions from known adjacent letters\n",
    "                if pos > 0 and masked_word[pos - 1] != '_':\n",
    "                    prev_letter = masked_word[pos - 1]\n",
    "                    prev_idx = self.letter_to_idx[prev_letter]\n",
    "                    letter_scores += model['transition_probs'][prev_idx] * 0.5\n",
    "                \n",
    "                if pos < length - 1 and masked_word[pos + 1] != '_':\n",
    "                    # Use reverse transition as approximation\n",
    "                    next_letter = masked_word[pos + 1]\n",
    "                    next_idx = self.letter_to_idx[next_letter]\n",
    "                    for i in range(26):\n",
    "                        letter_scores[i] += model['transition_probs'][i][next_idx] * 0.3\n",
    "        \n",
    "        # Zero out already guessed letters\n",
    "        for letter in guessed_letters:\n",
    "            if letter in self.letter_to_idx:\n",
    "                letter_scores[self.letter_to_idx[letter]] = 0\n",
    "        \n",
    "        # Normalize\n",
    "        if letter_scores.sum() > 0:\n",
    "            letter_scores = letter_scores / letter_scores.sum()\n",
    "        else:\n",
    "            # Uniform distribution over unguessed letters\n",
    "            letter_scores = np.ones(26)\n",
    "            for letter in guessed_letters:\n",
    "                if letter in self.letter_to_idx:\n",
    "                    letter_scores[self.letter_to_idx[letter]] = 0\n",
    "            if letter_scores.sum() > 0:\n",
    "                letter_scores = letter_scores / letter_scores.sum()\n",
    "        \n",
    "        return letter_scores\n",
    "    \n",
    "    def _get_frequency_based_probs(self, guessed_letters):\n",
    "        \"\"\"Fallback: general English letter frequency.\"\"\"\n",
    "        freq = np.array([\n",
    "            0.08167, 0.01492, 0.02782, 0.04253, 0.12702, 0.02228, 0.02015,\n",
    "            0.06094, 0.06966, 0.00153, 0.00772, 0.04025, 0.02406, 0.06749,\n",
    "            0.07507, 0.01929, 0.00095, 0.05987, 0.06327, 0.09056, 0.02758,\n",
    "            0.00978, 0.02360, 0.00150, 0.01974, 0.00074\n",
    "        ])\n",
    "        \n",
    "        for letter in guessed_letters:\n",
    "            if letter in self.letter_to_idx:\n",
    "                freq[self.letter_to_idx[letter]] = 0\n",
    "        \n",
    "        if freq.sum() > 0:\n",
    "            freq = freq / freq.sum()\n",
    "        \n",
    "        return freq\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save trained model to file.\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.models, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        \"\"\"Load trained model from file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.models = pickle.load(f)\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "\n",
    "\n",
    "# Training script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Hangman HMM Training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize and train HMM\n",
    "    hmm = HangmanHMM()\n",
    "    hmm.train('corpus.txt')\n",
    "    \n",
    "    # Save the trained model\n",
    "    hmm.save('hangman_hmm_model.pkl')\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Testing HMM\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"_ppl_\", set(['e', 's'])),\n",
    "        (\"h_ll_\", set(['e'])),\n",
    "        (\"_____\", set()),\n",
    "        (\"pro___m\", set(['a', 'e', 'i']))\n",
    "    ]\n",
    "    \n",
    "    for masked_word, guessed in test_cases:\n",
    "        probs = hmm.get_letter_probabilities(masked_word, guessed)\n",
    "        \n",
    "        # Get top 5 letters\n",
    "        top_indices = np.argsort(probs)[-5:][::-1]\n",
    "        top_letters = [(hmm.idx_to_letter[idx], probs[idx]) for idx in top_indices]\n",
    "        \n",
    "        print(f\"\\nMasked word: {masked_word}\")\n",
    "        print(f\"Guessed: {guessed}\")\n",
    "        print(f\"Top 5 predictions: {[(l, f'{p:.4f}') for l, p in top_letters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c28b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DQN Agent...\n",
      "DQN chose action: l\n",
      "\n",
      "Testing Q-Learning Agent...\n",
      "Q-Learning chose action: c\n",
      "\n",
      "Agents initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, defaultdict\n",
    "import pickle\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Hangman.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=55, action_size=26):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class HangmanDQNAgent:\n",
    "    \"\"\"DQN Agent for Hangman.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size=55, action_size=26, learning_rate=0.0005,\n",
    "                 gamma=0.95, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.9995,\n",
    "                 memory_size=10000, batch_size=64):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Main network\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        \n",
    "        # Target network\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Experience replay\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.target_update_freq = 100\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights from main model to target model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def act(self, state, valid_actions, use_hmm_probs=True):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current game state (dict with 'hmm_probs' and other info)\n",
    "            valid_actions: List of valid action letters\n",
    "            use_hmm_probs: Whether to combine Q-values with HMM probabilities\n",
    "            \n",
    "        Returns:\n",
    "            action: Letter to guess\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        # Get state vector for neural network\n",
    "        # Need to create state vector from state dict\n",
    "        state_vector = self._state_to_vector(state)\n",
    "        \n",
    "        # Get Q-values from model\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state_vector).unsqueeze(0).to(self.device)\n",
    "            q_values = self.model(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Combine with HMM probabilities if available\n",
    "        if use_hmm_probs and 'hmm_probs' in state:\n",
    "            hmm_probs = state['hmm_probs']\n",
    "            # Normalize Q-values\n",
    "            q_norm = (q_values - q_values.min()) / (q_values.max() - q_values.min() + 1e-8)\n",
    "            # Weighted combination\n",
    "            combined = 0.7 * q_norm + 0.3 * hmm_probs * 100\n",
    "        else:\n",
    "            combined = q_values\n",
    "        \n",
    "        # Mask invalid actions\n",
    "        valid_indices = [ord(a) - ord('a') for a in valid_actions]\n",
    "        masked_values = np.full(26, -np.inf)\n",
    "        masked_values[valid_indices] = combined[valid_indices]\n",
    "        \n",
    "        # Choose best action\n",
    "        action_idx = np.argmax(masked_values)\n",
    "        return chr(action_idx + ord('a'))\n",
    "    \n",
    "    def _state_to_vector(self, state):\n",
    "        \"\"\"Convert state dict to vector for neural network.\"\"\"\n",
    "        # Guessed letters (26 binary features)\n",
    "        guessed_vec = np.zeros(26)\n",
    "        for letter in state['guessed_letters']:\n",
    "            idx = ord(letter) - ord('a')\n",
    "            guessed_vec[idx] = 1\n",
    "        \n",
    "        # Lives remaining (normalized)\n",
    "        lives_vec = np.array([state['lives_remaining'] / 6.0])\n",
    "        \n",
    "        # HMM probabilities (26 features)\n",
    "        hmm_vec = state.get('hmm_probs', np.zeros(26))\n",
    "        \n",
    "        # Word length (normalized)\n",
    "        length_vec = np.array([state['word_length'] / 20.0])\n",
    "        \n",
    "        # Number of blanks (normalized)\n",
    "        blanks_vec = np.array([state['num_blanks'] / state['word_length']])\n",
    "        \n",
    "        # Concatenate\n",
    "        state_vector = np.concatenate([\n",
    "            guessed_vec,\n",
    "            lives_vec,\n",
    "            hmm_vec,\n",
    "            length_vec,\n",
    "            blanks_vec\n",
    "        ])\n",
    "        \n",
    "        return state_vector\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        state_vec = self._state_to_vector(state)\n",
    "        next_state_vec = self._state_to_vector(next_state)\n",
    "        action_idx = ord(action) - ord('a')\n",
    "        self.memory.push(state_vec, action_idx, reward, next_state_vec, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train on batch from replay memory.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states)).to(self.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.model(states).gather(1, actions).squeeze()\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_model(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.steps += 1\n",
    "        if self.steps % self.target_update_freq == 0:\n",
    "            self.update_target_model()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save model to file.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'target_model_state_dict': self.target_model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'steps': self.steps\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        print(f\"DQN model saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        \"\"\"Load model from file.\"\"\"\n",
    "        checkpoint = torch.load(filename, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.epsilon = checkpoint.get('epsilon', self.epsilon_min)\n",
    "        self.steps = checkpoint.get('steps', 0)\n",
    "        print(f\"DQN model loaded from {filename}\")\n",
    "\n",
    "\n",
    "class SimpleQLearningAgent:\n",
    "    \"\"\"Table-based Q-Learning Agent for Hangman.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.95, epsilon=1.0, \n",
    "                 epsilon_min=0.05, epsilon_decay=0.9995):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Q-table: dict of (state_key, action) -> Q-value\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        self.action_space = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    def _get_state_key(self, state):\n",
    "        \"\"\"Convert state to hashable key for Q-table.\"\"\"\n",
    "        masked_word = state['masked_word']\n",
    "        guessed = ''.join(sorted(state['guessed_letters']))\n",
    "        lives = state['lives_remaining']\n",
    "        return f\"{masked_word}:{guessed}:{lives}\"\n",
    "    \n",
    "    def act(self, state, valid_actions, use_hmm_probs=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
    "        state_key = self._get_state_key(state)\n",
    "        \n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        # Get Q-values for valid actions\n",
    "        q_values = {action: self.q_table[state_key][action] \n",
    "                   for action in valid_actions}\n",
    "        \n",
    "        # Combine with HMM probabilities if available\n",
    "        if use_hmm_probs and 'hmm_probs' in state:\n",
    "            hmm_probs = state['hmm_probs']\n",
    "            for action in valid_actions:\n",
    "                action_idx = ord(action) - ord('a')\n",
    "                hmm_score = hmm_probs[action_idx] * 100\n",
    "                q_values[action] = 0.7 * q_values[action] + 0.3 * hmm_score\n",
    "        \n",
    "        # Choose best action\n",
    "        return max(q_values, key=q_values.get)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value for state-action pair.\"\"\"\n",
    "        state_key = self._get_state_key(state)\n",
    "        next_state_key = self._get_state_key(next_state)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        \n",
    "        # Maximum Q-value for next state\n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            next_state_dict = self.q_table[next_state_key]\n",
    "            max_next_q = max(next_state_dict.values()) if next_state_dict else 0\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state_key][action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save Q-table to file.\"\"\"\n",
    "        data = {\n",
    "            'q_table': dict(self.q_table),\n",
    "            'epsilon': self.epsilon,\n",
    "            'alpha': self.alpha,\n",
    "            'gamma': self.gamma\n",
    "        }\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Q-Learning model saved to {filename}\")\n",
    "    \n",
    "    def load(self, filename):\n",
    "        \"\"\"Load Q-table from file.\"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Convert back to defaultdict\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        for state_key, actions in data['q_table'].items():\n",
    "            for action, q_value in actions.items():\n",
    "                self.q_table[state_key][action] = q_value\n",
    "        \n",
    "        self.epsilon = data.get('epsilon', self.epsilon_min)\n",
    "        self.alpha = data.get('alpha', self.alpha)\n",
    "        self.gamma = data.get('gamma', self.gamma)\n",
    "        print(f\"Q-Learning model loaded from {filename}\")\n",
    "\n",
    "\n",
    "# Test the agents\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing DQN Agent...\")\n",
    "    dqn_agent = HangmanDQNAgent()\n",
    "    \n",
    "    # Create dummy state\n",
    "    test_state = {\n",
    "        'masked_word': '_pp__',\n",
    "        'guessed_letters': set(['e', 's']),\n",
    "        'lives_remaining': 5,\n",
    "        'hmm_probs': np.random.rand(26),\n",
    "        'word_length': 5,\n",
    "        'num_blanks': 3\n",
    "    }\n",
    "    \n",
    "    valid_actions = ['a', 'b', 'c', 'd', 'l', 'o']\n",
    "    action = dqn_agent.act(test_state, valid_actions)\n",
    "    print(f\"DQN chose action: {action}\")\n",
    "    \n",
    "    print(\"\\nTesting Q-Learning Agent...\")\n",
    "    ql_agent = SimpleQLearningAgent()\n",
    "    action = ql_agent.act(test_state, valid_actions)\n",
    "    print(f\"Q-Learning chose action: {action}\")\n",
    "    \n",
    "    print(\"\\nAgents initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class HangmanEnvironment:\n",
    "    \"\"\"\n",
    "    Hangman game environment for Reinforcement Learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, word_list, hmm_model, max_lives=6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_list: List of words to use for games\n",
    "            hmm_model: Trained HMM model for probability predictions\n",
    "            max_lives: Maximum number of wrong guesses allowed\n",
    "        \"\"\"\n",
    "        self.word_list = word_list\n",
    "        self.hmm = hmm_model\n",
    "        self.max_lives = max_lives\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        \n",
    "        # Current game state\n",
    "        self.target_word = None\n",
    "        self.masked_word = None\n",
    "        self.guessed_letters = None\n",
    "        self.lives_remaining = None\n",
    "        self.wrong_guesses = 0\n",
    "        self.repeated_guesses = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Start a new game with a random word.\"\"\"\n",
    "        self.target_word = random.choice(self.word_list).lower()\n",
    "        self.masked_word = ['_'] * len(self.target_word)\n",
    "        self.guessed_letters = set()\n",
    "        self.lives_remaining = self.max_lives\n",
    "        self.wrong_guesses = 0\n",
    "        self.repeated_guesses = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Get current state representation.\n",
    "        \n",
    "        Returns dictionary containing:\n",
    "        - masked_word: string representation\n",
    "        - guessed_letters: set of guessed letters\n",
    "        - lives_remaining: int\n",
    "        - hmm_probs: probability distribution from HMM\n",
    "        - word_length: int\n",
    "        \"\"\"\n",
    "        masked_str = ''.join(self.masked_word)\n",
    "        hmm_probs = self.hmm.get_letter_probabilities(masked_str, self.guessed_letters)\n",
    "        \n",
    "        state = {\n",
    "            'masked_word': masked_str,\n",
    "            'guessed_letters': self.guessed_letters.copy(),\n",
    "            'lives_remaining': self.lives_remaining,\n",
    "            'hmm_probs': hmm_probs,\n",
    "            'word_length': len(self.target_word),\n",
    "            'num_blanks': masked_str.count('_')\n",
    "        }\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def get_state_vector(self):\n",
    "        \"\"\"\n",
    "        Convert state to fixed-size vector for neural network input.\n",
    "        \n",
    "        Returns:\n",
    "            numpy array suitable for DQN\n",
    "        \"\"\"\n",
    "        state = self._get_state()\n",
    "        \n",
    "        # Components:\n",
    "        # 1. Guessed letters (26 binary features)\n",
    "        guessed_vec = np.zeros(26)\n",
    "        for letter in state['guessed_letters']:\n",
    "            idx = ord(letter) - ord('a')\n",
    "            guessed_vec[idx] = 1\n",
    "        \n",
    "        # 2. Lives remaining (normalized)\n",
    "        lives_vec = np.array([state['lives_remaining'] / self.max_lives])\n",
    "        \n",
    "        # 3. HMM probabilities (26 features)\n",
    "        hmm_vec = state['hmm_probs']\n",
    "        \n",
    "        # 4. Word length (normalized)\n",
    "        length_vec = np.array([state['word_length'] / 20.0])  # Assuming max length ~20\n",
    "        \n",
    "        # 5. Number of blanks (normalized)\n",
    "        blanks_vec = np.array([state['num_blanks'] / state['word_length']])\n",
    "        \n",
    "        # Concatenate all features\n",
    "        state_vector = np.concatenate([\n",
    "            guessed_vec,    # 26\n",
    "            lives_vec,      # 1\n",
    "            hmm_vec,        # 26\n",
    "            length_vec,     # 1\n",
    "            blanks_vec      # 1\n",
    "        ])  # Total: 55 features\n",
    "        \n",
    "        return state_vector\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (guess a letter).\n",
    "        \n",
    "        Args:\n",
    "            action: letter to guess (string or int index)\n",
    "            \n",
    "        Returns:\n",
    "            next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        # Convert action to letter if it's an index\n",
    "        if isinstance(action, int):\n",
    "            letter = chr(action + ord('a'))\n",
    "        else:\n",
    "            letter = action.lower()\n",
    "        \n",
    "        # Check if letter was already guessed\n",
    "        if letter in self.guessed_letters:\n",
    "            self.repeated_guesses += 1\n",
    "            reward = -50  # Heavy penalty for repeated guess\n",
    "            done = False\n",
    "            info = {'repeated': True, 'wrong': False}\n",
    "            return self._get_state(), reward, done, info\n",
    "        \n",
    "        # Add to guessed letters\n",
    "        self.guessed_letters.add(letter)\n",
    "        \n",
    "        # Check if letter is in the word\n",
    "        if letter in self.target_word:\n",
    "            # Correct guess - reveal letters\n",
    "            count = 0\n",
    "            for i, char in enumerate(self.target_word):\n",
    "                if char == letter:\n",
    "                    self.masked_word[i] = letter\n",
    "                    count += 1\n",
    "            \n",
    "            # Check if word is complete\n",
    "            if '_' not in self.masked_word:\n",
    "                reward = 100 + (self.lives_remaining * 10)  # Win bonus + life bonus\n",
    "                done = True\n",
    "                info = {'won': True, 'wrong': False, 'letters_revealed': count}\n",
    "            else:\n",
    "                reward = 10 * count  # Reward proportional to letters revealed\n",
    "                done = False\n",
    "                info = {'won': False, 'wrong': False, 'letters_revealed': count}\n",
    "        else:\n",
    "            # Wrong guess\n",
    "            self.lives_remaining -= 1\n",
    "            self.wrong_guesses += 1\n",
    "            \n",
    "            if self.lives_remaining <= 0:\n",
    "                reward = -100  # Loss penalty\n",
    "                done = True\n",
    "                info = {'won': False, 'wrong': True, 'lost': True}\n",
    "            else:\n",
    "                reward = -20  # Wrong guess penalty\n",
    "                done = False\n",
    "                info = {'won': False, 'wrong': True, 'lost': False}\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"Get list of letters that haven't been guessed yet.\"\"\"\n",
    "        valid = []\n",
    "        for letter in self.alphabet:\n",
    "            if letter not in self.guessed_letters:\n",
    "                valid.append(letter)\n",
    "        return valid\n",
    "    \n",
    "    def get_valid_action_indices(self):\n",
    "        \"\"\"Get indices of valid actions (for neural network output).\"\"\"\n",
    "        valid = []\n",
    "        for i, letter in enumerate(self.alphabet):\n",
    "            if letter not in self.guessed_letters:\n",
    "                valid.append(i)\n",
    "        return valid\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Print current game state.\"\"\"\n",
    "        print(f\"Word: {' '.join(self.masked_word)}\")\n",
    "        print(f\"Guessed: {sorted(self.guessed_letters)}\")\n",
    "        print(f\"Lives: {self.lives_remaining}/{self.max_lives}\")\n",
    "        print(f\"Wrong guesses: {self.wrong_guesses}\")\n",
    "\n",
    "\n",
    "def load_corpus(filename):\n",
    "    \"\"\"Load word list from corpus file.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        words = [line.strip().lower() for line in f if line.strip()]\n",
    "    \n",
    "    # Filter only alphabetic words\n",
    "    words = [w for w in words if w.isalpha() and len(w) >= 2]\n",
    "    return words\n",
    "\n",
    "\n",
    "# Test the environment\n",
    "if __name__ == \"__main__\":\n",
    "    from hangman_hmm import HangmanHMM\n",
    "    \n",
    "    print(\"Loading HMM model...\")\n",
    "    hmm = HangmanHMM()\n",
    "    hmm.load('hangman_hmm_model.pkl')\n",
    "    \n",
    "    print(\"Loading corpus...\")\n",
    "    words = load_corpus('corpus.txt')\n",
    "    print(f\"Loaded {len(words)} words\")\n",
    "    \n",
    "    print(\"\\nTesting environment...\")\n",
    "    env = HangmanEnvironment(words, hmm)\n",
    "    \n",
    "    # Play a test game\n",
    "    state = env.reset()\n",
    "    print(f\"\\nTarget word (hidden): {env.target_word}\")\n",
    "    env.render()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        # Get HMM recommendation\n",
    "        hmm_probs = state['hmm_probs']\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        \n",
    "        # Choose letter with highest probability\n",
    "        best_idx = np.argmax(hmm_probs)\n",
    "        guess = chr(best_idx + ord('a'))\n",
    "        \n",
    "        # Make sure it's valid\n",
    "        if guess not in valid_actions:\n",
    "            guess = random.choice(valid_actions)\n",
    "        \n",
    "        print(f\"\\nGuessing: {guess}\")\n",
    "        state, reward, done, info = env.step(guess)\n",
    "        print(f\"Reward: {reward}, Info: {info}\")\n",
    "        env.render()\n",
    "    \n",
    "    if info.get('won'):\n",
    "        print(\"\\nüéâ Won the game!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Lost! Word was: {env.target_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bcfa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HANGMAN RL AGENT - TRAINING\n",
      "============================================================\n",
      "\n",
      "1. Loading HMM model...\n",
      "Model loaded from hangman_hmm_model.pkl\n",
      "   ‚úì HMM loaded successfully\n",
      "\n",
      "2. Loading corpus...\n",
      "   ‚úì Loaded 49933 words\n",
      "\n",
      "3. Creating Hangman environment...\n",
      "   ‚úì Environment created\n",
      "\n",
      "4. Choose agent type:\n",
      "   1. DQN (Deep Q-Network) - More powerful, slower\n",
      "\n",
      "5. Initializing DQN agent...\n",
      "   ‚úì DQN agent initialized\n",
      "\n",
      "6. Training configuration:\n",
      "   Recommended episodes for DQN: 5000\n",
      "\n",
      "   Training for 5000 episodes...\n",
      "   Estimated time: 16.7 minutes\n",
      "\n",
      "============================================================\n",
      "Training DQN Agent\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñà         | 502/5000 [00:14<03:25, 21.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 500/5000\n",
      "  Win rate (last 500): 0.60%\n",
      "  Avg wrong guesses: 5.99\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -162.88\n",
      "  Epsilon: 0.7788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|‚ñà‚ñà        | 1002/5000 [00:38<03:04, 21.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1000/5000\n",
      "  Win rate (last 500): 2.40%\n",
      "  Avg wrong guesses: 5.95\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -146.88\n",
      "  Epsilon: 0.6065\n",
      "DQN model saved to hangman_dqn_checkpoint_1000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|‚ñà‚ñà‚ñà       | 1505/5000 [01:00<02:03, 28.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1500/5000\n",
      "  Win rate (last 500): 9.00%\n",
      "  Avg wrong guesses: 5.82\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -120.04\n",
      "  Epsilon: 0.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 2002/5000 [01:17<01:53, 26.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 2000/5000\n",
      "  Win rate (last 500): 7.40%\n",
      "  Avg wrong guesses: 5.84\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -119.94\n",
      "  Epsilon: 0.3678\n",
      "DQN model saved to hangman_dqn_checkpoint_2000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2503/5000 [01:42<01:52, 22.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 2500/5000\n",
      "  Win rate (last 500): 12.00%\n",
      "  Avg wrong guesses: 5.76\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -104.94\n",
      "  Epsilon: 0.2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3005/5000 [02:03<00:51, 38.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 3000/5000\n",
      "  Win rate (last 500): 18.40%\n",
      "  Avg wrong guesses: 5.62\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -87.74\n",
      "  Epsilon: 0.2230\n",
      "DQN model saved to hangman_dqn_checkpoint_3000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3503/5000 [02:24<00:57, 25.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 3500/5000\n",
      "  Win rate (last 500): 18.80%\n",
      "  Avg wrong guesses: 5.62\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -83.14\n",
      "  Epsilon: 0.1737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4003/5000 [02:44<00:48, 20.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 4000/5000\n",
      "  Win rate (last 500): 21.60%\n",
      "  Avg wrong guesses: 5.52\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -74.50\n",
      "  Epsilon: 0.1353\n",
      "DQN model saved to hangman_dqn_checkpoint_4000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4504/5000 [03:06<00:17, 28.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 4500/5000\n",
      "  Win rate (last 500): 24.60%\n",
      "  Avg wrong guesses: 5.50\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -67.02\n",
      "  Epsilon: 0.1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [03:26<00:00, 24.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 5000/5000\n",
      "  Win rate (last 500): 24.00%\n",
      "  Avg wrong guesses: 5.44\n",
      "  Avg repeated guesses: 0.000\n",
      "  Avg reward: -65.22\n",
      "  Epsilon: 0.0820\n",
      "DQN model saved to hangman_dqn_checkpoint_5000.pth\n",
      "\n",
      "8. Saving final model...\n",
      "DQN model saved to hangman_dqn_final.pth\n",
      "   Training history saved to training_history_dqn_20251103_154929.json\n",
      "\n",
      "9. Generating training plots...\n",
      "\n",
      "Training plots saved to training_results_dqn_20251103_154929.png\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Final Statistics:\n",
      "  Final win rate: 24.00%\n",
      "  Final avg wrong guesses: 5.44\n",
      "  Final avg repeated guesses: 0.000\n",
      "  Final epsilon: 0.0820\n",
      "\n",
      "Saved files:\n",
      "  - Model: hangman_dqn_final.pth\n",
      "  - History: training_history_dqn_20251103_154929.json\n",
      "  - Plots: training_results_dqn_20251103_154929.png\n",
      "\n",
      "Next step: Evaluate your agent\n",
      "  Run: python hangman_evaluation.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training script for Hangman RL Agent.\n",
    "Supports both DQN and Q-Learning agents.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from hangman_hmm import HangmanHMM\n",
    "from hangman_environment import HangmanEnvironment, load_corpus\n",
    "from hangman_agent import HangmanDQNAgent, SimpleQLearningAgent\n",
    "\n",
    "def smooth_curve(values, window=100):\n",
    "    \"\"\"Smooth curve using moving average.\"\"\"\n",
    "    if len(values) < window:\n",
    "        return values\n",
    "    smoothed = []\n",
    "    for i in range(len(values)):\n",
    "        start = max(0, i - window // 2)\n",
    "        end = min(len(values), i + window // 2)\n",
    "        smoothed.append(np.mean(values[start:end]))\n",
    "    return smoothed\n",
    "\n",
    "def plot_training_results(history, filename='training_results.png'):\n",
    "    \"\"\"Plot training metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    episodes = range(len(history['rewards']))\n",
    "    \n",
    "    # 1. Episode Rewards (smoothed)\n",
    "    axes[0, 0].plot(episodes, history['rewards'], alpha=0.3, label='Raw')\n",
    "    smoothed = smooth_curve(history['rewards'])\n",
    "    axes[0, 0].plot(episodes, smoothed, label='Smoothed', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Total Reward')\n",
    "    axes[0, 0].set_title('Episode Rewards Over Time')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Win Rate\n",
    "    win_rate_episodes = range(0, len(history['win_rates']) * 500, 500)\n",
    "    axes[0, 1].plot(win_rate_episodes, history['win_rates'], \n",
    "                    marker='o', linewidth=2, markersize=4)\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Win Rate (%)')\n",
    "    axes[0, 1].set_title('Win Rate Over Time (per 500 episodes)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 100])\n",
    "    \n",
    "    # 3. Wrong Guesses\n",
    "    axes[1, 0].plot(win_rate_episodes, history['avg_wrong_guesses'], \n",
    "                    marker='s', color='orange', linewidth=2, markersize=4)\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Avg Wrong Guesses')\n",
    "    axes[1, 0].set_title('Wrong Guesses Over Time (per 500 episodes)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Repeated Guesses\n",
    "    axes[1, 1].plot(win_rate_episodes, history['avg_repeated_guesses'], \n",
    "                    marker='^', color='red', linewidth=2, markersize=4)\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Avg Repeated Guesses')\n",
    "    axes[1, 1].set_title('Repeated Guesses Over Time (per 500 episodes)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nTraining plots saved to {filename}\")\n",
    "    plt.close()\n",
    "\n",
    "def train_dqn(env, agent, num_episodes=5000, save_freq=1000):\n",
    "    \"\"\"Train DQN agent.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training DQN Agent\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    history = {\n",
    "        'rewards': [],\n",
    "        'losses': [],\n",
    "        'win_rates': [],\n",
    "        'avg_wrong_guesses': [],\n",
    "        'avg_repeated_guesses': [],\n",
    "        'epsilons': []\n",
    "    }\n",
    "    \n",
    "    # Tracking variables\n",
    "    episode_wins = []\n",
    "    episode_wrong = []\n",
    "    episode_repeated = []\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            action = agent.act(state, valid_actions, use_hmm_probs=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train on batch\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                loss = agent.replay()\n",
    "                losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record episode stats\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['losses'].extend(losses)\n",
    "        history['epsilons'].append(agent.epsilon)\n",
    "        \n",
    "        episode_wins.append(1 if info.get('won') else 0)\n",
    "        episode_wrong.append(env.wrong_guesses)\n",
    "        episode_repeated.append(env.repeated_guesses)\n",
    "        \n",
    "        # Log progress every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            win_rate = np.mean(episode_wins[-500:]) * 100\n",
    "            avg_wrong = np.mean(episode_wrong[-500:])\n",
    "            avg_repeated = np.mean(episode_repeated[-500:])\n",
    "            avg_reward = np.mean(history['rewards'][-500:])\n",
    "            \n",
    "            history['win_rates'].append(win_rate)\n",
    "            history['avg_wrong_guesses'].append(avg_wrong)\n",
    "            history['avg_repeated_guesses'].append(avg_repeated)\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"  Win rate (last 500): {win_rate:.2f}%\")\n",
    "            print(f\"  Avg wrong guesses: {avg_wrong:.2f}\")\n",
    "            print(f\"  Avg repeated guesses: {avg_repeated:.3f}\")\n",
    "            print(f\"  Avg reward: {avg_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (episode + 1) % save_freq == 0:\n",
    "            checkpoint_file = f'hangman_dqn_checkpoint_{episode + 1}.pth'\n",
    "            agent.save(checkpoint_file)\n",
    "    \n",
    "    return history\n",
    "\n",
    "def train_qlearning(env, agent, num_episodes=10000):\n",
    "    \"\"\"Train Q-Learning agent.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Q-Learning Agent\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    history = {\n",
    "        'rewards': [],\n",
    "        'win_rates': [],\n",
    "        'avg_wrong_guesses': [],\n",
    "        'avg_repeated_guesses': [],\n",
    "        'epsilons': []\n",
    "    }\n",
    "    \n",
    "    # Tracking variables\n",
    "    episode_wins = []\n",
    "    episode_wrong = []\n",
    "    episode_repeated = []\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            action = agent.act(state, valid_actions, use_hmm_probs=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Learn\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record episode stats\n",
    "        history['rewards'].append(total_reward)\n",
    "        history['epsilons'].append(agent.epsilon)\n",
    "        \n",
    "        episode_wins.append(1 if info.get('won') else 0)\n",
    "        episode_wrong.append(env.wrong_guesses)\n",
    "        episode_repeated.append(env.repeated_guesses)\n",
    "        \n",
    "        # Log progress every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            win_rate = np.mean(episode_wins[-500:]) * 100\n",
    "            avg_wrong = np.mean(episode_wrong[-500:])\n",
    "            avg_repeated = np.mean(episode_repeated[-500:])\n",
    "            avg_reward = np.mean(history['rewards'][-500:])\n",
    "            \n",
    "            history['win_rates'].append(win_rate)\n",
    "            history['avg_wrong_guesses'].append(avg_wrong)\n",
    "            history['avg_repeated_guesses'].append(avg_repeated)\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"  Win rate (last 500): {win_rate:.2f}%\")\n",
    "            print(f\"  Avg wrong guesses: {avg_wrong:.2f}\")\n",
    "            print(f\"  Avg repeated guesses: {avg_repeated:.3f}\")\n",
    "            print(f\"  Avg reward: {avg_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "            print(f\"  Q-table size: {len(agent.q_table)}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"HANGMAN RL AGENT - TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load HMM\n",
    "    print(\"\\n1. Loading HMM model...\")\n",
    "    try:\n",
    "        hmm = HangmanHMM()\n",
    "        hmm.load('hangman_hmm_model.pkl')\n",
    "        print(\"   ‚úì HMM loaded successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ùå HMM model not found!\")\n",
    "        print(\"   Please run 'python hangman_hmm.py' first to train the HMM.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Load corpus\n",
    "    print(\"\\n2. Loading corpus...\")\n",
    "    try:\n",
    "        words = load_corpus('corpus.txt')\n",
    "        print(f\"   ‚úì Loaded {len(words)} words\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ùå corpus.txt not found!\")\n",
    "        return\n",
    "    \n",
    "    # 3. Create environment\n",
    "    print(\"\\n3. Creating Hangman environment...\")\n",
    "    env = HangmanEnvironment(words, hmm, max_lives=6)\n",
    "    print(\"   ‚úì Environment created\")\n",
    "    \n",
    "    # 4. Choose agent type\n",
    "    print(\"\\n4. Choose agent type:\")\n",
    "    print(\"   1. DQN (Deep Q-Network) - More powerful, slower\")\n",
    "    print(\"   2. Q-Learning (Table-based) - Simpler, faster\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "        if choice in ['1', '2']:\n",
    "            break\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "    \n",
    "    use_dqn = (choice == '1')\n",
    "    \n",
    "    # 5. Initialize agent\n",
    "    if use_dqn:\n",
    "        print(\"\\n5. Initializing DQN agent...\")\n",
    "        agent = HangmanDQNAgent(\n",
    "            state_size=55,\n",
    "            action_size=26,\n",
    "            learning_rate=0.0005,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_min=0.05,\n",
    "            epsilon_decay=0.9995,\n",
    "            memory_size=10000,\n",
    "            batch_size=64\n",
    "        )\n",
    "        agent_name = \"DQN\"\n",
    "        default_episodes = 5000\n",
    "    else:\n",
    "        print(\"\\n5. Initializing Q-Learning agent...\")\n",
    "        agent = SimpleQLearningAgent(\n",
    "            alpha=0.1,\n",
    "            gamma=0.95,\n",
    "            epsilon=1.0,\n",
    "            epsilon_min=0.05,\n",
    "            epsilon_decay=0.9995\n",
    "        )\n",
    "        agent_name = \"Q-Learning\"\n",
    "        default_episodes = 10000\n",
    "    \n",
    "    print(f\"   ‚úì {agent_name} agent initialized\")\n",
    "    \n",
    "    # 6. Get training parameters\n",
    "    print(f\"\\n6. Training configuration:\")\n",
    "    print(f\"   Recommended episodes for {agent_name}: {default_episodes}\")\n",
    "    \n",
    "    while True:\n",
    "        episodes_input = input(f\"   Enter number of episodes (or press Enter for {default_episodes}): \").strip()\n",
    "        if episodes_input == \"\":\n",
    "            num_episodes = default_episodes\n",
    "            break\n",
    "        try:\n",
    "            num_episodes = int(episodes_input)\n",
    "            if num_episodes > 0:\n",
    "                break\n",
    "            print(\"   Please enter a positive number.\")\n",
    "        except ValueError:\n",
    "            print(\"   Invalid input. Please enter a number.\")\n",
    "    \n",
    "    print(f\"\\n   Training for {num_episodes} episodes...\")\n",
    "    print(f\"   Estimated time: {num_episodes * 0.2 / 60:.1f} minutes\")\n",
    "    \n",
    "    # 7. Train agent\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if use_dqn:\n",
    "        history = train_dqn(env, agent, num_episodes, save_freq=1000)\n",
    "        model_file = f'hangman_dqn_final.pth'\n",
    "    else:\n",
    "        history = train_qlearning(env, agent, num_episodes)\n",
    "        model_file = f'hangman_qlearning_final.pkl'\n",
    "    \n",
    "    # 8. Save final model\n",
    "    print(f\"\\n8. Saving final model...\")\n",
    "    agent.save(model_file)\n",
    "    \n",
    "    # 9. Save training history\n",
    "    history_file = f'training_history_{agent_name.lower()}_{timestamp}.json'\n",
    "    with open(history_file, 'w') as f:\n",
    "        # Convert numpy arrays to lists for JSON serialization\n",
    "        history_json = {k: [float(v) for v in vals] for k, vals in history.items()}\n",
    "        json.dump(history_json, f, indent=2)\n",
    "    print(f\"   Training history saved to {history_file}\")\n",
    "    \n",
    "    # 10. Plot results\n",
    "    print(\"\\n9. Generating training plots...\")\n",
    "    plot_file = f'training_results_{agent_name.lower()}_{timestamp}.png'\n",
    "    plot_training_results(history, plot_file)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nFinal Statistics:\")\n",
    "    if history['win_rates']:\n",
    "        print(f\"  Final win rate: {history['win_rates'][-1]:.2f}%\")\n",
    "        print(f\"  Final avg wrong guesses: {history['avg_wrong_guesses'][-1]:.2f}\")\n",
    "        print(f\"  Final avg repeated guesses: {history['avg_repeated_guesses'][-1]:.3f}\")\n",
    "    print(f\"  Final epsilon: {history['epsilons'][-1]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nSaved files:\")\n",
    "    print(f\"  - Model: {model_file}\")\n",
    "    print(f\"  - History: {history_file}\")\n",
    "    print(f\"  - Plots: {plot_file}\")\n",
    "    \n",
    "    print(f\"\\nNext step: Evaluate your agent\")\n",
    "    print(f\"  Run: python hangman_evaluation.py\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d696658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HANGMAN RL AGENT - EVALUATION\n",
      "============================================================\n",
      "\n",
      "1. Loading HMM model...\n",
      "Model loaded from hangman_hmm_model.pkl\n",
      "   ‚úì HMM loaded\n",
      "\n",
      "2. Loading corpus...\n",
      "   ‚úì Loaded 9987 test words\n",
      "\n",
      "3. Creating evaluation environment...\n",
      "   ‚úì Environment created\n",
      "\n",
      "4. Choose agent to evaluate:\n",
      "   1. DQN\n",
      "   2. Q-Learning\n",
      "\n",
      "5. Loading DQN agent...\n",
      "DQN model loaded from hangman_dqn_final.pth\n",
      "   ‚úì Agent loaded (epsilon set to 0 for evaluation)\n",
      "\n",
      "6. Evaluation settings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajpu\\Desktop\\hangman2\\hangman2\\hangman_agent.py:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Evaluating agent on 2000 games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 21/2000 [00:00<00:09, 198.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Game 1 ===\n",
      "Target word: jovicentrical\n",
      "Turn 1: Guessed 'i' -> ['_', '_', '_', 'i', '_', '_', '_', '_', '_', 'i', '_', '_', '_']\n",
      "Turn 2: Guessed 'o' -> ['_', 'o', '_', 'i', '_', '_', '_', '_', '_', 'i', '_', '_', '_']\n",
      "Turn 3: Guessed 'n' -> ['_', 'o', '_', 'i', '_', '_', 'n', '_', '_', 'i', '_', '_', '_']\n",
      "Turn 4: Guessed 't' -> ['_', 'o', '_', 'i', '_', '_', 'n', 't', '_', 'i', '_', '_', '_']\n",
      "Turn 5: Guessed 'e' -> ['_', 'o', '_', 'i', '_', 'e', 'n', 't', '_', 'i', '_', '_', '_']\n",
      "Turn 6: Guessed 's' -> ['_', 'o', '_', 'i', '_', 'e', 'n', 't', '_', 'i', '_', '_', '_']\n",
      "Turn 7: Guessed 'r' -> ['_', 'o', '_', 'i', '_', 'e', 'n', 't', 'r', 'i', '_', '_', '_']\n",
      "Turn 8: Guessed 'c' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', '_', '_']\n",
      "Turn 9: Guessed 'l' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', '_', 'l']\n",
      "Turn 10: Guessed 'a' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "Turn 11: Guessed 'p' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "Turn 12: Guessed 'm' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "Turn 13: Guessed 'd' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "Turn 14: Guessed 'u' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "Turn 15: Guessed 'h' -> ['_', 'o', '_', 'i', 'c', 'e', 'n', 't', 'r', 'i', 'c', 'a', 'l']\n",
      "‚úó Lost! Word was: jovicentrical\n",
      "\n",
      "=== Game 2 ===\n",
      "Target word: fractionize\n",
      "Turn 1: Guessed 'e' -> ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e']\n",
      "Turn 2: Guessed 'i' -> ['_', '_', '_', '_', '_', 'i', '_', '_', 'i', '_', 'e']\n",
      "Turn 3: Guessed 'n' -> ['_', '_', '_', '_', '_', 'i', '_', 'n', 'i', '_', 'e']\n",
      "Turn 4: Guessed 'o' -> ['_', '_', '_', '_', '_', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 5: Guessed 't' -> ['_', '_', '_', '_', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 6: Guessed 'a' -> ['_', '_', 'a', '_', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 7: Guessed 'r' -> ['_', 'r', 'a', '_', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 8: Guessed 's' -> ['_', 'r', 'a', '_', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 9: Guessed 'c' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 10: Guessed 'p' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 11: Guessed 'l' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 12: Guessed 'v' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 13: Guessed 'u' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "Turn 14: Guessed 'd' -> ['_', 'r', 'a', 'c', 't', 'i', 'o', 'n', 'i', '_', 'e']\n",
      "‚úó Lost! Word was: fractionize\n",
      "\n",
      "=== Game 3 ===\n",
      "Target word: proholiday\n",
      "Turn 1: Guessed 'e' -> ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_']\n",
      "Turn 2: Guessed 'i' -> ['_', '_', '_', '_', '_', '_', 'i', '_', '_', '_']\n",
      "Turn 3: Guessed 'o' -> ['_', '_', 'o', '_', 'o', '_', 'i', '_', '_', '_']\n",
      "Turn 4: Guessed 'n' -> ['_', '_', 'o', '_', 'o', '_', 'i', '_', '_', '_']\n",
      "Turn 5: Guessed 'r' -> ['_', 'r', 'o', '_', 'o', '_', 'i', '_', '_', '_']\n",
      "Turn 6: Guessed 't' -> ['_', 'r', 'o', '_', 'o', '_', 'i', '_', '_', '_']\n",
      "Turn 7: Guessed 's' -> ['_', 'r', 'o', '_', 'o', '_', 'i', '_', '_', '_']\n",
      "Turn 8: Guessed 'a' -> ['_', 'r', 'o', '_', 'o', '_', 'i', '_', 'a', '_']\n",
      "Turn 9: Guessed 'c' -> ['_', 'r', 'o', '_', 'o', '_', 'i', '_', 'a', '_']\n",
      "Turn 10: Guessed 'l' -> ['_', 'r', 'o', '_', 'o', 'l', 'i', '_', 'a', '_']\n",
      "Turn 11: Guessed 'p' -> ['p', 'r', 'o', '_', 'o', 'l', 'i', '_', 'a', '_']\n",
      "Turn 12: Guessed 'd' -> ['p', 'r', 'o', '_', 'o', 'l', 'i', 'd', 'a', '_']\n",
      "Turn 13: Guessed 'm' -> ['p', 'r', 'o', '_', 'o', 'l', 'i', 'd', 'a', '_']\n",
      "‚úó Lost! Word was: proholiday\n",
      "\n",
      "=== Game 4 ===\n",
      "Target word: photoceptor\n",
      "Turn 1: Guessed 'e' -> ['_', '_', '_', '_', '_', '_', 'e', '_', '_', '_', '_']\n",
      "Turn 2: Guessed 'i' -> ['_', '_', '_', '_', '_', '_', 'e', '_', '_', '_', '_']\n",
      "Turn 3: Guessed 'n' -> ['_', '_', '_', '_', '_', '_', 'e', '_', '_', '_', '_']\n",
      "Turn 4: Guessed 'r' -> ['_', '_', '_', '_', '_', '_', 'e', '_', '_', '_', 'r']\n",
      "Turn 5: Guessed 'o' -> ['_', '_', 'o', '_', 'o', '_', 'e', '_', '_', 'o', 'r']\n",
      "Turn 6: Guessed 't' -> ['_', '_', 'o', 't', 'o', '_', 'e', '_', 't', 'o', 'r']\n",
      "Turn 7: Guessed 'a' -> ['_', '_', 'o', 't', 'o', '_', 'e', '_', 't', 'o', 'r']\n",
      "Turn 8: Guessed 's' -> ['_', '_', 'o', 't', 'o', '_', 'e', '_', 't', 'o', 'r']\n",
      "Turn 9: Guessed 'c' -> ['_', '_', 'o', 't', 'o', 'c', 'e', '_', 't', 'o', 'r']\n",
      "Turn 10: Guessed 'p' -> ['p', '_', 'o', 't', 'o', 'c', 'e', 'p', 't', 'o', 'r']\n",
      "Turn 11: Guessed 'h' -> ['p', 'h', 'o', 't', 'o', 'c', 'e', 'p', 't', 'o', 'r']\n",
      "‚úì Won! (4 wrong guesses)\n",
      "\n",
      "=== Game 5 ===\n",
      "Target word: wallach\n",
      "Turn 1: Guessed 'e' -> ['_', '_', '_', '_', '_', '_', '_']\n",
      "Turn 2: Guessed 'a' -> ['_', 'a', '_', '_', 'a', '_', '_']\n",
      "Turn 3: Guessed 'n' -> ['_', 'a', '_', '_', 'a', '_', '_']\n",
      "Turn 4: Guessed 'r' -> ['_', 'a', '_', '_', 'a', '_', '_']\n",
      "Turn 5: Guessed 't' -> ['_', 'a', '_', '_', 'a', '_', '_']\n",
      "Turn 6: Guessed 'l' -> ['_', 'a', 'l', 'l', 'a', '_', '_']\n",
      "Turn 7: Guessed 's' -> ['_', 'a', 'l', 'l', 'a', '_', '_']\n",
      "Turn 8: Guessed 'i' -> ['_', 'a', 'l', 'l', 'a', '_', '_']\n",
      "‚úó Lost! Word was: wallach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:10<00:00, 192.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Games Played: 2000\n",
      "Games Won: 519\n",
      "Games Lost: 1481\n",
      "\n",
      "Success Rate: 25.95%\n",
      "Perfect Games (0 wrong): 6 (0.3%)\n",
      "\n",
      "Total Wrong Guesses: 10760\n",
      "Avg Wrong Guesses per Game: 5.380\n",
      "\n",
      "Total Repeated Guesses: 0\n",
      "Avg Repeated Guesses per Game: 0.000\n",
      "\n",
      "Avg Game Length: 11.3 turns\n",
      "\n",
      "------------------------------------------------------------\n",
      "FINAL SCORE CALCULATION\n",
      "------------------------------------------------------------\n",
      "Success Rate √ó 2000 = 519.00\n",
      "Wrong Guesses √ó 5 = 53800.00\n",
      "Repeated Guesses √ó 2 = 0.00\n",
      "\n",
      "FINAL SCORE: -53281.00\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TOP 20 MOST DIFFICULT WORDS\n",
      "============================================================\n",
      "\n",
      "1. 'JOVICENTRICAL'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 15\n",
      "\n",
      "2. 'FRACTIONIZE'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 14\n",
      "\n",
      "3. 'PROHOLIDAY'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 13\n",
      "\n",
      "4. 'WALLACH'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 8\n",
      "\n",
      "5. 'KEXY'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 7\n",
      "\n",
      "6. 'PREVACCINATE'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 14\n",
      "\n",
      "7. 'STANNOTYPE'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 12\n",
      "\n",
      "8. 'HANDBILL'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 10\n",
      "\n",
      "9. 'OUTTHROUGH'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 9\n",
      "\n",
      "10. 'FIENDHEAD'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 12\n",
      "\n",
      "11. 'TOWAI'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 10\n",
      "\n",
      "12. 'INTERVIEWER'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 12\n",
      "\n",
      "13. 'RETROPOSED'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 12\n",
      "\n",
      "14. 'ONCORHYNCHUS'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 12\n",
      "\n",
      "15. 'ACIERAGE'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 10\n",
      "\n",
      "16. 'CHEMIOTROPISM'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 15\n",
      "\n",
      "17. 'HABERDASHER'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 9\n",
      "\n",
      "18. 'PUNNAGE'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 9\n",
      "\n",
      "19. 'EUPHEMIZER'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 9\n",
      "\n",
      "20. 'PREPENSELY'\n",
      "   Wrong guesses: 6\n",
      "   Repeated guesses: 0\n",
      "   Total turns: 11\n",
      "\n",
      "============================================================\n",
      "DIFFICULTY BY WORD LENGTH\n",
      "============================================================\n",
      "Length  2:   1 failed games, avg 6.00 wrong guesses\n",
      "Length  3:  14 failed games, avg 6.00 wrong guesses\n",
      "Length  4:  21 failed games, avg 6.00 wrong guesses\n",
      "Length  5:  78 failed games, avg 6.00 wrong guesses\n",
      "Length  6: 123 failed games, avg 6.00 wrong guesses\n",
      "Length  7: 174 failed games, avg 6.00 wrong guesses\n",
      "Length  8: 219 failed games, avg 6.00 wrong guesses\n",
      "Length  9: 243 failed games, avg 6.00 wrong guesses\n",
      "Length 10: 191 failed games, avg 6.00 wrong guesses\n",
      "Length 11: 153 failed games, avg 6.00 wrong guesses\n",
      "Length 12: 127 failed games, avg 6.00 wrong guesses\n",
      "Length 13:  67 failed games, avg 6.00 wrong guesses\n",
      "Length 14:  37 failed games, avg 6.00 wrong guesses\n",
      "Length 15:  22 failed games, avg 6.00 wrong guesses\n",
      "Length 16:   9 failed games, avg 6.00 wrong guesses\n",
      "Length 17:   2 failed games, avg 6.00 wrong guesses\n",
      "\n",
      "Results saved to evaluation_results_dqn_20251103_152335.json\n",
      "\n",
      "11. Generating evaluation plots...\n",
      "\n",
      "Evaluation plots saved to evaluation_results_dqn_20251103_152335.png\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Your estimated grade: D/F\n",
      "Final Score: -53281.00\n",
      "\n",
      "Generated files:\n",
      "  - Results: evaluation_results_dqn_20251103_152335.json\n",
      "  - Plots: evaluation_results_dqn_20251103_152335.png\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation script for Hangman RL Agent.\n",
    "Tests agent performance and generates detailed analysis.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from hangman_hmm import HangmanHMM\n",
    "from hangman_environment import HangmanEnvironment, load_corpus\n",
    "from hangman_agent import HangmanDQNAgent, SimpleQLearningAgent\n",
    "\n",
    "def evaluate_agent(env, agent, num_games=2000, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate agent on test games.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'games_played': 0,\n",
    "        'games_won': 0,\n",
    "        'games_lost': 0,\n",
    "        'total_wrong_guesses': 0,\n",
    "        'total_repeated_guesses': 0,\n",
    "        'wrong_guesses_per_game': [],\n",
    "        'repeated_guesses_per_game': [],\n",
    "        'game_lengths': [],\n",
    "        'difficult_words': [],  # Words that caused losses\n",
    "        'perfect_games': 0,  # Won with 0 wrong guesses\n",
    "    }\n",
    "    \n",
    "    for game_num in tqdm(range(num_games), desc=\"Evaluating\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        num_turns = 0\n",
    "        \n",
    "        if verbose and game_num < 5:\n",
    "            print(f\"\\n=== Game {game_num + 1} ===\")\n",
    "            print(f\"Target word: {env.target_word}\")\n",
    "        \n",
    "        while not done:\n",
    "            num_turns += 1\n",
    "            \n",
    "            # Get action from agent (greedy, no exploration)\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            action = agent.act(state, valid_actions, use_hmm_probs=True)\n",
    "            \n",
    "            # Take action\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            if verbose and game_num < 5:\n",
    "                print(f\"Turn {num_turns}: Guessed '{action}' -> {env.masked_word}\")\n",
    "        \n",
    "        # Record results\n",
    "        results['games_played'] += 1\n",
    "        \n",
    "        if info.get('won'):\n",
    "            results['games_won'] += 1\n",
    "            if env.wrong_guesses == 0:\n",
    "                results['perfect_games'] += 1\n",
    "        else:\n",
    "            results['games_lost'] += 1\n",
    "            results['difficult_words'].append({\n",
    "                'word': env.target_word,\n",
    "                'wrong_guesses': env.wrong_guesses,\n",
    "                'repeated_guesses': env.repeated_guesses,\n",
    "                'turns': num_turns\n",
    "            })\n",
    "        \n",
    "        results['total_wrong_guesses'] += env.wrong_guesses\n",
    "        results['total_repeated_guesses'] += env.repeated_guesses\n",
    "        results['wrong_guesses_per_game'].append(env.wrong_guesses)\n",
    "        results['repeated_guesses_per_game'].append(env.repeated_guesses)\n",
    "        results['game_lengths'].append(num_turns)\n",
    "        \n",
    "        if verbose and game_num < 5:\n",
    "            if info.get('won'):\n",
    "                print(f\"‚úì Won! ({env.wrong_guesses} wrong guesses)\")\n",
    "            else:\n",
    "                print(f\"‚úó Lost! Word was: {env.target_word}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_evaluation_results(results, filename='evaluation_results.png'):\n",
    "    \"\"\"Generate evaluation plots.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Wrong Guesses Distribution\n",
    "    axes[0, 0].hist(results['wrong_guesses_per_game'], bins=range(0, 8), \n",
    "                    edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Wrong Guesses per Game')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Wrong Guesses')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Repeated Guesses Distribution\n",
    "    max_repeated = max(results['repeated_guesses_per_game']) + 1\n",
    "    axes[0, 1].hist(results['repeated_guesses_per_game'], \n",
    "                    bins=range(0, max_repeated + 1), \n",
    "                    edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_xlabel('Repeated Guesses per Game')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Repeated Guesses')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Win/Loss Pie Chart\n",
    "    labels = ['Won', 'Lost']\n",
    "    sizes = [results['games_won'], results['games_lost']]\n",
    "    colors = ['#90EE90', '#FFB6C6']\n",
    "    explode = (0.05, 0)\n",
    "    \n",
    "    axes[1, 0].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                   autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "    axes[1, 0].set_title('Win/Loss Ratio')\n",
    "    \n",
    "    # 4. Performance Metrics Summary\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    success_rate = results['games_won'] / results['games_played'] * 100\n",
    "    avg_wrong = results['total_wrong_guesses'] / results['games_played']\n",
    "    avg_repeated = results['total_repeated_guesses'] / results['games_played']\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    Performance Summary\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    Games Played:           {results['games_played']:,}\n",
    "    Games Won:              {results['games_won']:,}\n",
    "    Games Lost:             {results['games_lost']:,}\n",
    "    \n",
    "    Success Rate:           {success_rate:.2f}%\n",
    "    Perfect Games:          {results['perfect_games']} ({results['perfect_games']/results['games_played']*100:.1f}%)\n",
    "    \n",
    "    Total Wrong Guesses:    {results['total_wrong_guesses']:,}\n",
    "    Avg Wrong/Game:         {avg_wrong:.3f}\n",
    "    \n",
    "    Total Repeated Guesses: {results['total_repeated_guesses']:,}\n",
    "    Avg Repeated/Game:      {avg_repeated:.3f}\n",
    "    \n",
    "    Avg Game Length:        {np.mean(results['game_lengths']):.1f} turns\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, \n",
    "                    verticalalignment='center', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nEvaluation plots saved to {filename}\")\n",
    "    plt.close()\n",
    "\n",
    "def analyze_difficult_words(results, top_n=20):\n",
    "    \"\"\"Analyze most difficult words.\"\"\"\n",
    "    if not results['difficult_words']:\n",
    "        print(\"\\nNo failed games to analyze!\")\n",
    "        return\n",
    "    \n",
    "    # Sort by wrong guesses\n",
    "    sorted_words = sorted(results['difficult_words'], \n",
    "                         key=lambda x: x['wrong_guesses'], \n",
    "                         reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TOP {top_n} MOST DIFFICULT WORDS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, word_info in enumerate(sorted_words[:top_n], 1):\n",
    "        print(f\"\\n{i}. '{word_info['word'].upper()}'\")\n",
    "        print(f\"   Wrong guesses: {word_info['wrong_guesses']}\")\n",
    "        print(f\"   Repeated guesses: {word_info['repeated_guesses']}\")\n",
    "        print(f\"   Total turns: {word_info['turns']}\")\n",
    "    \n",
    "    # Word length analysis\n",
    "    word_lengths = defaultdict(list)\n",
    "    for word_info in results['difficult_words']:\n",
    "        length = len(word_info['word'])\n",
    "        word_lengths[length].append(word_info['wrong_guesses'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIFFICULTY BY WORD LENGTH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for length in sorted(word_lengths.keys()):\n",
    "        avg_wrong = np.mean(word_lengths[length])\n",
    "        count = len(word_lengths[length])\n",
    "        print(f\"Length {length:2d}: {count:3d} failed games, \"\n",
    "              f\"avg {avg_wrong:.2f} wrong guesses\")\n",
    "\n",
    "def calculate_final_score(results):\n",
    "    \"\"\"Calculate final score according to competition formula.\"\"\"\n",
    "    success_rate = results['games_won'] / results['games_played']\n",
    "    total_wrong = results['total_wrong_guesses']\n",
    "    total_repeated = results['total_repeated_guesses']\n",
    "    \n",
    "    score = (success_rate * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def print_evaluation_summary(results):\n",
    "    \"\"\"Print detailed evaluation summary.\"\"\"\n",
    "    success_rate = results['games_won'] / results['games_played'] * 100\n",
    "    avg_wrong = results['total_wrong_guesses'] / results['games_played']\n",
    "    avg_repeated = results['total_repeated_guesses'] / results['games_played']\n",
    "    final_score = calculate_final_score(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nGames Played: {results['games_played']}\")\n",
    "    print(f\"Games Won: {results['games_won']}\")\n",
    "    print(f\"Games Lost: {results['games_lost']}\")\n",
    "    \n",
    "    print(f\"\\nSuccess Rate: {success_rate:.2f}%\")\n",
    "    print(f\"Perfect Games (0 wrong): {results['perfect_games']} \"\n",
    "          f\"({results['perfect_games']/results['games_played']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal Wrong Guesses: {results['total_wrong_guesses']}\")\n",
    "    print(f\"Avg Wrong Guesses per Game: {avg_wrong:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTotal Repeated Guesses: {results['total_repeated_guesses']}\")\n",
    "    print(f\"Avg Repeated Guesses per Game: {avg_repeated:.3f}\")\n",
    "    \n",
    "    print(f\"\\nAvg Game Length: {np.mean(results['game_lengths']):.1f} turns\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"FINAL SCORE CALCULATION\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"Success Rate √ó 2000 = {success_rate/100 * 2000:.2f}\")\n",
    "    print(f\"Wrong Guesses √ó 5 = {results['total_wrong_guesses'] * 5:.2f}\")\n",
    "    print(f\"Repeated Guesses √ó 2 = {results['total_repeated_guesses'] * 2:.2f}\")\n",
    "    print(\"\")\n",
    "    print(f\"FINAL SCORE: {final_score:.2f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"HANGMAN RL AGENT - EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load HMM\n",
    "    print(\"\\n1. Loading HMM model...\")\n",
    "    try:\n",
    "        hmm = HangmanHMM()\n",
    "        hmm.load('hangman_hmm_model.pkl')\n",
    "        print(\"   ‚úì HMM loaded\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ùå HMM model not found!\")\n",
    "        return\n",
    "    \n",
    "    # 2. Load corpus\n",
    "    print(\"\\n2. Loading corpus...\")\n",
    "    try:\n",
    "        all_words = load_corpus('corpus.txt')\n",
    "        # Use last 20% for testing\n",
    "        test_start = int(len(all_words) * 0.8)\n",
    "        test_words = all_words[test_start:]\n",
    "        print(f\"   ‚úì Loaded {len(test_words)} test words\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚ùå corpus.txt not found!\")\n",
    "        return\n",
    "    \n",
    "    # 3. Create environment\n",
    "    print(\"\\n3. Creating evaluation environment...\")\n",
    "    env = HangmanEnvironment(test_words, hmm, max_lives=6)\n",
    "    print(\"   ‚úì Environment created\")\n",
    "    \n",
    "    # 4. Choose agent type\n",
    "    print(\"\\n4. Choose agent to evaluate:\")\n",
    "    print(\"   1. DQN\")\n",
    "    print(\"   2. Q-Learning\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nEnter choice (1 or 2): \").strip()\n",
    "        if choice in ['1', '2']:\n",
    "            break\n",
    "        print(\"Invalid choice.\")\n",
    "    \n",
    "    use_dqn = (choice == '1')\n",
    "    \n",
    "    # 5. Load agent\n",
    "    print(f\"\\n5. Loading {'DQN' if use_dqn else 'Q-Learning'} agent...\")\n",
    "    \n",
    "    if use_dqn:\n",
    "        agent = HangmanDQNAgent()\n",
    "        default_file = 'hangman_dqn_final.pth'\n",
    "    else:\n",
    "        agent = SimpleQLearningAgent()\n",
    "        default_file = 'hangman_qlearning_final.pkl'\n",
    "    \n",
    "    model_file = input(f\"   Model file (or press Enter for '{default_file}'): \").strip()\n",
    "    if not model_file:\n",
    "        model_file = default_file\n",
    "    \n",
    "    try:\n",
    "        agent.load(model_file)\n",
    "        agent.epsilon = 0.0  # Greedy evaluation, no exploration\n",
    "        print(f\"   ‚úì Agent loaded (epsilon set to 0 for evaluation)\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ùå Model file '{model_file}' not found!\")\n",
    "        return\n",
    "    \n",
    "    # 6. Evaluation parameters\n",
    "    print(\"\\n6. Evaluation settings:\")\n",
    "    num_games_input = input(\"   Number of games (press Enter for 2000): \").strip()\n",
    "    num_games = int(num_games_input) if num_games_input else 2000\n",
    "    \n",
    "    verbose_input = input(\"   Verbose output for first 5 games? (y/n, default n): \").strip().lower()\n",
    "    verbose = (verbose_input == 'y')\n",
    "    \n",
    "    # 7. Run evaluation\n",
    "    print(f\"\\n7. Evaluating agent on {num_games} games...\")\n",
    "    results = evaluate_agent(env, agent, num_games, verbose)\n",
    "    \n",
    "    # 8. Print summary\n",
    "    print_evaluation_summary(results)\n",
    "    \n",
    "    # 9. Analyze difficult words\n",
    "    if results['games_lost'] > 0:\n",
    "        analyze_difficult_words(results, top_n=20)\n",
    "    \n",
    "    # 10. Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    agent_name = \"dqn\" if use_dqn else \"qlearning\"\n",
    "    \n",
    "    results_file = f'evaluation_results_{agent_name}_{timestamp}.json'\n",
    "    \n",
    "    # Convert results to JSON-serializable format\n",
    "    results_json = {\n",
    "        'games_played': results['games_played'],\n",
    "        'games_won': results['games_won'],\n",
    "        'games_lost': results['games_lost'],\n",
    "        'success_rate': results['games_won'] / results['games_played'] * 100,\n",
    "        'total_wrong_guesses': results['total_wrong_guesses'],\n",
    "        'total_repeated_guesses': results['total_repeated_guesses'],\n",
    "        'avg_wrong_guesses': results['total_wrong_guesses'] / results['games_played'],\n",
    "        'avg_repeated_guesses': results['total_repeated_guesses'] / results['games_played'],\n",
    "        'perfect_games': results['perfect_games'],\n",
    "        'final_score': float(calculate_final_score(results)),\n",
    "        'difficult_words': results['difficult_words'][:50]  # Save top 50\n",
    "    }\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_json, f, indent=2)\n",
    "    print(f\"\\nResults saved to {results_file}\")\n",
    "    \n",
    "    # 11. Generate plots\n",
    "    print(\"\\n11. Generating evaluation plots...\")\n",
    "    plot_file = f'evaluation_results_{agent_name}_{timestamp}.png'\n",
    "    plot_evaluation_results(results, plot_file)\n",
    "    \n",
    "    # Final message\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    final_score = calculate_final_score(results)\n",
    "    \n",
    "    if final_score > 1500:\n",
    "        grade = \"A+\"\n",
    "    elif final_score > 1000:\n",
    "        grade = \"A\"\n",
    "    elif final_score > 500:\n",
    "        grade = \"B\"\n",
    "    elif final_score > 0:\n",
    "        grade = \"C\"\n",
    "    else:\n",
    "        grade = \"D/F\"\n",
    "    \n",
    "    print(f\"\\nYour estimated grade: {grade}\")\n",
    "    print(f\"Final Score: {final_score:.2f}\")\n",
    "    \n",
    "    print(f\"\\nGenerated files:\")\n",
    "    print(f\"  - Results: {results_file}\")\n",
    "    print(f\"  - Plots: {plot_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
